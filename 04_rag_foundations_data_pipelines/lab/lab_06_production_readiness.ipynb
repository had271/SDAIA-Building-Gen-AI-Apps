{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c868e95",
   "metadata": {},
   "source": [
    "# Lab 6: Production Readiness\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will:\n",
    "- Benchmark HNSW parameters systematically (M, ef_construction) using ChromaDB\n",
    "- Understand quantization trade-offs (SQ8, PQ) for scaling\n",
    "- Implement a zero-downtime reindexing strategy with collection aliasing\n",
    "- Estimate vector database costs with growth projections\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f0f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install chromadb numpy matplotlib -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68d05d",
   "metadata": {},
   "source": [
    "## Part 1: HNSW Benchmarking\n",
    "\n",
    "HNSW (Hierarchical Navigable Small World) is the engine behind most vector databases. In this section, we move from simulation to real benchmarking using **ChromaDB**.\n",
    "\n",
    "First, let's initialize a persistent ChromaDB client so we can \"see\" the data on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b1b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import os\n",
    "\n",
    "# Initialize persistent client\n",
    "db_path = \"./chroma_db_files\"\n",
    "client = chromadb.PersistentClient(path=db_path)\n",
    "\n",
    "# Create a sample collection to \"see\" it working\n",
    "sample_coll = client.get_or_create_collection(name=\"verify_chroma\")\n",
    "sample_coll.add(\n",
    "    embeddings=[[0.1] * 384],\n",
    "    ids=[\"verify_doc\"]\n",
    ")\n",
    "\n",
    "print(f\"ChromaDB initialized at: {os.path.abspath(db_path)}\")\n",
    "print(f\"Collections: {client.list_collections()}\")\n",
    "print(f\"Verification document: {sample_coll.get()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a1788",
   "metadata": {},
   "source": [
    "### Systematic Parameter Sweeping\n",
    "\n",
    "Now we benchmark how `hnsw:M` and `hnsw:construction_ef` impact search latency and recall. We calculate **real recall** against a brute-force baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f09547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "def benchmark_chroma_hnsw(n_vectors, dim, M_values, ef_construction_values):\n",
    "    \"\"\"Benchmark real ChromaDB HNSW parameters with real recall calculation.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Generate test data (normalized for cosine)\n",
    "    vectors = np.random.randn(n_vectors, dim).astype('float32')\n",
    "    vectors /= np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    queries = np.random.randn(50, dim).astype('float32')\n",
    "    queries /= np.linalg.norm(queries, axis=1, keepdims=True)\n",
    "    \n",
    "    # 0. Build Brute-Force Baseline for Recall\n",
    "    print(f\"Calculating brute-force baseline for {len(queries)} queries...\")\n",
    "    ground_truth = []\n",
    "    for q in queries:\n",
    "        similarities = np.dot(vectors, q)\n",
    "        top_k_idx = np.argsort(similarities)[::-1][:10]\n",
    "        ground_truth.append(set(top_k_idx.astype(str)))\n",
    "    \n",
    "    print(f\"Starting ChromaDB benchmark on {n_vectors} vectors...\")\n",
    "    \n",
    "    for M in M_values:\n",
    "        for efc in ef_construction_values:\n",
    "            coll_name = f\"bench_{M}_{efc}_{uuid.uuid4().hex[:8]}\"\n",
    "            \n",
    "            # 1. Measure Build Time\n",
    "            start_build = time.perf_counter()\n",
    "            collection = client.create_collection(\n",
    "                name=coll_name,\n",
    "                metadata={\n",
    "                    \"hnsw:space\": \"cosine\",\n",
    "                    \"hnsw:M\": M,\n",
    "                    \"hnsw:construction_ef\": efc\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            collection.add(\n",
    "                embeddings=vectors.tolist(),\n",
    "                ids=[str(i) for i in range(n_vectors)]\n",
    "            )\n",
    "            build_time = time.perf_counter() - start_build\n",
    "            \n",
    "            # 2. Measure Search Latency & Recall\n",
    "            search_times = []\n",
    "            hits = 0\n",
    "            for i, q in enumerate(queries):\n",
    "                start_q = time.perf_counter()\n",
    "                res = collection.query(query_embeddings=[q.tolist()], n_results=10)\n",
    "                search_times.append((time.perf_counter() - start_q) * 1000)\n",
    "                \n",
    "                # Calculate real recall\n",
    "                retrieved_ids = set(res['ids'][0])\n",
    "                hits += len(retrieved_ids.intersection(ground_truth[i]))\n",
    "            \n",
    "            avg_recall = hits / (len(queries) * 10)\n",
    "            \n",
    "            # 3. Rough Memory Calculation\n",
    "            memory_mb = (n_vectors * dim * 4 + n_vectors * M * 8) / (1024**2)\n",
    "            \n",
    "            results.append({\n",
    "                'M': M, \n",
    "                'ef_construction': efc,\n",
    "                'search_ms': np.mean(search_times),\n",
    "                'build_time_s': build_time,\n",
    "                'memory_mb': memory_mb,\n",
    "                'recall_at_10': avg_recall\n",
    "            })\n",
    "            \n",
    "            client.delete_collection(coll_name)\n",
    "            print(f\"  - Config [M={M}, efc={efc}] -> Search: {np.mean(search_times):.2f}ms, Recall: {avg_recall:.3f}\")\n",
    "            \n",
    "    return results\n",
    "\n",
    "M_values = [16, 32, 64]\n",
    "ef_construction_values = [100, 200]\n",
    "results = benchmark_chroma_hnsw(2000, 384, M_values, ef_construction_values)\n",
    "\n",
    "# Display results table\n",
    "print(f\"\\n{'M':>4} {'ef_const':>10} {'Search(ms)':>11} {'Recall@10':>10} {'Memory(MB)':>11}\")\n",
    "print(\"-\" * 55)\n",
    "for r in results:\n",
    "    print(f\"{r['M']:>4} {r['ef_construction']:>10} {r['search_ms']:>11.2f} {r['recall_at_10']:>10.3f} {r['memory_mb']:>11.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cad7a22",
   "metadata": {},
   "source": [
    "## Part 2: Visualize Trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a002043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "colors = {16: '#00C9A7', 32: '#FF7A5C', 64: '#1C355E'}\n",
    "\n",
    "# Plot 1: Speed vs Accuracy\n",
    "for M in M_values:\n",
    "    subset = [r for r in results if r['M'] == M]\n",
    "    subset.sort(key=lambda x: x['search_ms'])\n",
    "    ax1.plot([r['search_ms'] for r in subset], [r['recall_at_10'] for r in subset],              'o-', color=colors[M], label=f'M={M}', markersize=8)\n",
    "\n",
    "ax1.set_xlabel('Search Latency (ms)', fontsize=12)\n",
    "ax1.set_ylabel('Recall@10', fontsize=12)\n",
    "ax1.set_title('Speed vs Accuracy (Pareto Curve)', fontsize=14, color='#1C355E')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Memory Footprint\n",
    "for M in M_values:\n",
    "    subset = [r for r in results if r['M'] == M]\n",
    "    ax2.bar(str(M), subset[0]['memory_mb'], color=colors[M])\n",
    "\n",
    "ax2.set_xlabel('M Value', fontsize=12)\n",
    "ax2.set_ylabel('Estimated Memory (MB)', fontsize=12)\n",
    "ax2.set_title('Index Memory Footprint', fontsize=14, color='#1C355E')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d04ec",
   "metadata": {},
   "source": [
    "### Exercise 6.1: The Optimizer\n",
    "Write a function that selects the config with the highest recall that stays under a latency budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97685279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_config(results, max_search_ms=10.0, min_recall=0.94):\n",
    "    \"\"\"Find the best configuration within constraints.\"\"\"\n",
    "    valid = [r for r in results if r['search_ms'] <= max_search_ms and r['recall_at_10'] >= min_recall]\n",
    "    if valid:\n",
    "        return max(valid, key=lambda r: r['recall_at_10'])\n",
    "    else:\n",
    "        return min(results, key=lambda r: abs(r['search_ms'] - max_search_ms))\n",
    "\n",
    "optimal = find_optimal_config(results, max_search_ms=15.0, min_recall=0.94)\n",
    "from tests import checks\n",
    "checks.check_lab_6_3(optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ff02e9",
   "metadata": {},
   "source": [
    "## Part 3: Quantization Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f73fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory comparison for 1M vectors at 1536 dimensions\n",
    "n, d = 1_000_000, 1536\n",
    "index_types = [\n",
    "    {\"name\": \"Flat (F32)\", \"bytes\": 4, \"recall\": 1.0},\n",
    "    {\"name\": \"SQ8 (1-byte)\", \"bytes\": 1, \"recall\": 0.98},\n",
    "    {\"name\": \"PQ (bit-packed)\", \"bytes\": 0.25, \"recall\": 0.92},\n",
    "]\n",
    "\n",
    "print(f\"{'Index Type':<15} {'Memory (GB)':>12} {'Recall':>12}\")\n",
    "print(\"-\" * 45)\n",
    "for idx in index_types:\n",
    "    mem_gb = (n * d * idx['bytes']) / (1024**3)\n",
    "    print(f\"{idx['name']:<15} {mem_gb:>11.2f} {idx['recall']:>11.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5fe52a",
   "metadata": {},
   "source": [
    "## Part 4: Zero-Downtime Reindexing (Alias Switch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179394b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromaAliasManager:\n",
    "    \"\"\"Simulates a pointer to a live collection.\"\"\"\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self._live_name = None\n",
    "        \n",
    "    def get_live(self):\n",
    "        return self.client.get_collection(self._live_name) if self._live_name else None\n",
    "    \n",
    "    def switch(self, name):\n",
    "        print(f\"  [ALIAS] Pointing 'live' -> '{name}'\")\n",
    "        self._live_name = name\n",
    "\n",
    "class ReindexOrchestrator:\n",
    "    def __init__(self, client, alias):\n",
    "        self.client = client\n",
    "        self.alias = alias\n",
    "        \n",
    "    def reindex(self, data):\n",
    "        new_name = f\"research_assistant_{int(time.time())}\"\n",
    "        print(f\"\\nStarting Zero-Downtime Reindex...\")\n",
    "        shadow = self.client.create_collection(name=new_name)\n",
    "        shadow.add(embeddings=data, ids=[str(i) for i in range(len(data))])\n",
    "        self.alias.switch(new_name)\n",
    "        print(f\"  Atomic switch complete. Live collection: {new_name}\")\n",
    "\n",
    "# Re-use the existing client\n",
    "alias = ChromaAliasManager(client)\n",
    "orchestrator = ReindexOrchestrator(client, alias)\n",
    "\n",
    "# Initial deploy\n",
    "orchestrator.reindex(np.random.randn(50, 384).tolist())\n",
    "# Re-deploy\n",
    "time.sleep(1)\n",
    "orchestrator.reindex(np.random.randn(50, 384).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaa7b0b",
   "metadata": {},
   "source": [
    "## Part 5: Cost Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a11042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_cost(n_vecs, dim=1536, provider=\"pinecone\"):\n",
    "    rates = {\n",
    "        \"pinecone\": {\"gb\": 0.45, \"reads_m\": 1.20, \"writes_m\": 7.0},\n",
    "        \"weaviate\": {\"gb\": 0.20, \"reads_m\": 0.70, \"writes_m\": 2.5},\n",
    "        \"self_hosted\": {\"gb\": 0.10, \"reads_m\": 0.0, \"writes_m\": 0.0, \"base\": 100},\n",
    "    }\n",
    "    config = rates.get(provider, rates[\"pinecone\"])\n",
    "    storage_gb = (n_vecs * dim * 4) / (1024**3)\n",
    "    monthly_cost = storage_gb * config[\"gb\"] + (1.0 * config[\"reads_m\"]) + (0.1 * config[\"writes_m\"])\n",
    "    monthly_cost += config.get(\"base\", 0)\n",
    "    return monthly_cost\n",
    "\n",
    "scales = [100_000, 1_000_000, 10_000_000]\n",
    "print(f\"{'Vectors':>12} {'Pinecone':>12} {'Weaviate':>12} {'Self-Hosted':>12}\")\n",
    "for s in scales:\n",
    "    p = estimate_cost(s, provider=\"pinecone\")\n",
    "    w = estimate_cost(s, provider=\"weaviate\")\n",
    "    sh = estimate_cost(s, provider=\"self_hosted\")\n",
    "    print(f\"{s:>12,} ${p:>10.2f} ${w:>10.2f} ${sh:>11.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc4eefb",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "1. Why is an atomic alias switch better than just deleting and re-creating a collection?\n",
    "2. If your recall requirement is 99%, which HNSW parameter would you prioritize?\n",
    "3. What is the main cost driver for vector databases (Storage vs. Compute)?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
