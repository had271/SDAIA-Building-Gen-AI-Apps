{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Lab 4: Hybrid Search & Re-ranking\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will:\n",
    "- Implement BM25 keyword search alongside semantic search\n",
    "- Combine results using Reciprocal Rank Fusion (RRF)\n",
    "- Add Cross-Encoder re-ranking as a quality gate\n",
    "- Build a query preprocessing and expansion pipeline\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install rank-bm25 sentence-transformers numpy -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "## Part 1: Why Simple Search Fails\n",
    "\n",
    "Pure semantic search has **three key failure modes**:\n",
    "\n",
    "1. **Exact match blindness** \u2014 Semantic models may not prioritize documents containing the exact query terms (e.g., error codes like `503`, product IDs, or acronyms).\n",
    "2. **Out-of-domain vocabulary** \u2014 Embedding models trained on general text struggle with domain-specific jargon, rare terms, or newly coined words.\n",
    "3. **Over-generalization** \u2014 Semantic similarity can surface documents that are topically related but not actually relevant to the specific question.\n",
    "\n",
    "BM25 keyword search complements semantic search by excelling at exact term matching. Let's see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Demonstrate keyword search advantage\n",
    "corpus = [\n",
    "    \"Error 503: Service temporarily unavailable. Retry after 30 seconds.\",\n",
    "    \"The server experienced an internal problem and could not fulfill the request.\",\n",
    "    \"Network connectivity issues can cause service disruptions and timeouts.\",\n",
    "    \"HTTP status code 503 indicates the server is currently unable to handle the request.\",\n",
    "    \"Troubleshooting guide: when your application returns errors, check the logs first.\",\n",
    "]\n",
    "\n",
    "query = \"Error 503\"\n",
    "\n",
    "# BM25 (keyword) search\n",
    "tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "keyword_scores = bm25.get_scores(query.lower().split())\n",
    "\n",
    "# Simulated semantic search (random for demo)\n",
    "np.random.seed(42)\n",
    "semantic_scores = np.random.uniform(0.3, 0.8, len(corpus))\n",
    "# Boost docs that are semantically related but don't have exact keywords\n",
    "semantic_scores[1] = 0.85  # \"server problem\" is semantically similar\n",
    "semantic_scores[2] = 0.75  # \"service disruptions\" is related\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(f\"{'Doc':<5} {'BM25':>8} {'Semantic':>10} {'Text'}\")\n",
    "print(\"-\" * 80)\n",
    "for i, doc in enumerate(corpus):\n",
    "    print(f\"  {i+1:<3} {keyword_scores[i]:>8.3f} {semantic_scores[i]:>10.3f}   {doc[:60]}...\")\n",
    "\n",
    "print(f\"\\nBM25 top result: Doc {np.argmax(keyword_scores)+1} (exact match)\")\n",
    "print(f\"Semantic top result: Doc {np.argmax(semantic_scores)+1} (meaning match)\")\n",
    "print(\"\\nNeither alone finds all relevant documents!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e8a9b0",
   "metadata": {},
   "source": [
    "## Part 2: Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "Reciprocal Rank Fusion is a simple but effective technique for combining ranked lists from different retrieval methods. The key insight is that it only uses **rank positions**, not raw scores, making it robust to different score scales.\n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$\\text{RRF}(d) = \\sum_{r \\in R} \\frac{1}{k + \\text{rank}_r(d)}$$\n",
    "\n",
    "Where `k` is a smoothing constant (typically 60) and `rank_r(d)` is the rank of document `d` in ranked list `r`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e2f3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(ranked_lists, k=60):\n",
    "    \"\"\"\n",
    "    Combine multiple ranked result lists using RRF.\n",
    "    \n",
    "    RRF Score = sum(1 / (rank + k)) across all lists\n",
    "    \n",
    "    Args:\n",
    "        ranked_lists: List of lists, each containing (doc_id, score) tuples sorted by score desc\n",
    "        k: Smoothing constant (default 60)\n",
    "    \n",
    "    Returns:\n",
    "        List of (doc_id, fused_score) sorted by fused score descending\n",
    "    \"\"\"\n",
    "    fused_scores = {}\n",
    "    \n",
    "    for result_list in ranked_lists:\n",
    "        for rank, (doc_id, _score) in enumerate(result_list):\n",
    "            if doc_id not in fused_scores:\n",
    "                fused_scores[doc_id] = 0.0\n",
    "            fused_scores[doc_id] += 1.0 / (rank + k)\n",
    "    \n",
    "    # Sort by fused score descending\n",
    "    return sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Build ranked lists from our scores\n",
    "keyword_ranked = sorted(enumerate(keyword_scores), key=lambda x: x[1], reverse=True)\n",
    "keyword_ranked = [(i, s) for i, s in keyword_ranked]\n",
    "\n",
    "semantic_ranked = sorted(enumerate(semantic_scores), key=lambda x: x[1], reverse=True)\n",
    "semantic_ranked = [(i, s) for i, s in semantic_ranked]\n",
    "\n",
    "# Fuse!\n",
    "fused = reciprocal_rank_fusion([keyword_ranked, semantic_ranked], k=60)\n",
    "\n",
    "print(\"RRF Results:\")\n",
    "print(f\"{'Rank':<6} {'Doc':<5} {'RRF Score':>10} {'Text'}\")\n",
    "print(\"-\" * 70)\n",
    "for rank, (doc_id, score) in enumerate(fused, 1):\n",
    "    print(f\"  {rank:<4} {doc_id+1:<5} {score:>10.6f}   {corpus[doc_id][:55]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6d7e8",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Implement Weighted RRF\n",
    "\n",
    "Standard RRF treats all retrieval methods equally. But what if you know that semantic search is better for your domain? Implement a **weighted** version of RRF where each ranked list gets a configurable weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a0b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_rrf(ranked_lists, weights=None, k=60):\n",
    "    \"\"\"RRF with configurable weights per result list.\"\"\"\n",
    "    if weights is None:\n",
    "        weights = [1.0] * len(ranked_lists)\n",
    "    \n",
    "    # TODO: Implement weighted RRF\n",
    "    # Same as RRF, but multiply each list's contribution by its weight\n",
    "    # fused_scores[doc_id] += weight * (1 / (rank + k))\n",
    "    fused_scores = {}\n",
    "    \n",
    "    # Your implementation here\n",
    "    pass\n",
    "    \n",
    "    return sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Test: weight semantic search 2x more than keyword\n",
    "result = weighted_rrf([keyword_ranked, semantic_ranked], weights=[1.0, 2.0])\n",
    "from tests import checks\n",
    "checks.check_lab_4_2(result)\n",
    "\n",
    "# The semantic-heavy weighting should change the ranking\n",
    "print(\"Weighted RRF (semantic 2x):\")\n",
    "for rank, (doc_id, score) in enumerate(result[:3], 1):\n",
    "    print(f\"  {rank}. Doc {doc_id+1}: {score:.6f} - {corpus[doc_id][:60]}...\")\n",
    "print(\"Weighted RRF working!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d4e5f6",
   "metadata": {},
   "source": [
    "## Part 3: Cross-Encoder Re-ranking\n",
    "\n",
    "### Bi-Encoder vs. Cross-Encoder\n",
    "\n",
    "| | Bi-Encoder | Cross-Encoder |\n",
    "|---|---|---|\n",
    "| **How it works** | Encodes query and document independently, compares with cosine similarity | Encodes query and document together as a single input |\n",
    "| **Speed** | Fast (pre-compute embeddings) | Slow (must run for each query-document pair) |\n",
    "| **Accuracy** | Good | Better (sees full interaction between query and document) |\n",
    "| **Use case** | First-stage retrieval (search millions of docs) | Second-stage re-ranking (re-score top 10-50 candidates) |\n",
    "\n",
    "The typical pattern: use a bi-encoder (or hybrid search) to retrieve ~50 candidates, then use a cross-encoder to re-rank those candidates for the final top-K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c8d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleReranker:\n",
    "    \"\"\"\n",
    "    Simulated cross-encoder re-ranker.\n",
    "    In production: use sentence_transformers.CrossEncoder('BAAI/bge-reranker-base')\n",
    "    \"\"\"\n",
    "    \n",
    "    def rerank(self, query, candidates, top_k=3):\n",
    "        \"\"\"\n",
    "        Re-rank candidates based on query-document relevance.\n",
    "        \n",
    "        In production, this would use a CrossEncoder model:\n",
    "            pairs = [[query, doc['text']] for doc in candidates]\n",
    "            scores = cross_encoder.predict(pairs)\n",
    "        \"\"\"\n",
    "        scored = []\n",
    "        query_terms = set(query.lower().split())\n",
    "        \n",
    "        for doc in candidates:\n",
    "            text_terms = set(doc['text'].lower().split())\n",
    "            # Simulate relevance: overlap + length penalty\n",
    "            overlap = len(query_terms & text_terms)\n",
    "            length_penalty = min(1.0, 50 / max(len(doc['text']), 1))\n",
    "            score = overlap * 0.3 + (1 - length_penalty) * 0.7\n",
    "            scored.append({**doc, 'relevance_score': score})\n",
    "        \n",
    "        scored.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
    "        return scored[:top_k]\n",
    "\n",
    "# Demonstrate the full pipeline\n",
    "candidates = [\n",
    "    {\"id\": i, \"text\": doc, \"initial_score\": float(fused[j][1])}\n",
    "    for j, (i, _) in enumerate(fused)\n",
    "]\n",
    "\n",
    "reranker = SimpleReranker()\n",
    "final = reranker.rerank(\"Error 503\", candidates, top_k=3)\n",
    "\n",
    "print(\"After Re-ranking:\")\n",
    "for i, doc in enumerate(final, 1):\n",
    "    print(f\"  {i}. [relevance={doc['relevance_score']:.3f}] {doc['text'][:70]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4",
   "metadata": {},
   "source": [
    "## Part 4: Complete AdvancedRetriever\n",
    "\n",
    "Now let's put it all together into a single class that implements the full hybrid search + re-ranking pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRetriever:\n",
    "    \"\"\"Full hybrid search + re-ranking pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "        # Build BM25 index\n",
    "        tokenized = [doc['text'].lower().split() for doc in documents]\n",
    "        self.bm25 = BM25Okapi(tokenized)\n",
    "        self.reranker = SimpleReranker()\n",
    "    \n",
    "    def _keyword_search(self, query, top_k=10):\n",
    "        scores = self.bm25.get_scores(query.lower().split())\n",
    "        ranked_idx = np.argsort(scores)[::-1][:top_k]\n",
    "        return [(int(i), float(scores[i])) for i in ranked_idx if scores[i] > 0]\n",
    "    \n",
    "    def _semantic_search(self, query, top_k=10):\n",
    "        # Simulated - in production: embed query + vector store search\n",
    "        np.random.seed(hash(query) % 2**32)\n",
    "        scores = np.random.uniform(0.3, 0.9, len(self.documents))\n",
    "        ranked_idx = np.argsort(scores)[::-1][:top_k]\n",
    "        return [(int(i), float(scores[i])) for i in ranked_idx]\n",
    "    \n",
    "    def retrieve(self, query, top_k=3, candidates=10):\n",
    "        \"\"\"Full pipeline: hybrid search -> RRF -> re-rank -> top K.\"\"\"\n",
    "        # 1. Parallel retrieval\n",
    "        keyword_results = self._keyword_search(query, candidates)\n",
    "        semantic_results = self._semantic_search(query, candidates)\n",
    "        \n",
    "        # 2. Fuse with RRF\n",
    "        fused = reciprocal_rank_fusion([keyword_results, semantic_results])\n",
    "        \n",
    "        # 3. Get full documents for top candidates\n",
    "        candidate_docs = []\n",
    "        for doc_id, rrf_score in fused[:candidates]:\n",
    "            doc = {**self.documents[doc_id], 'id': doc_id, 'rrf_score': rrf_score}\n",
    "            candidate_docs.append(doc)\n",
    "        \n",
    "        # 4. Re-rank\n",
    "        final = self.reranker.rerank(query, candidate_docs, top_k=top_k)\n",
    "        \n",
    "        return final\n",
    "\n",
    "# Test\n",
    "docs = [{\"text\": doc} for doc in corpus]\n",
    "retriever = AdvancedRetriever(docs)\n",
    "results = retriever.retrieve(\"Error 503 service unavailable\", top_k=3)\n",
    "\n",
    "print(\"Advanced Retrieval Results:\")\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"  {i}. [score={r['relevance_score']:.3f}] {r['text'][:70]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g5h6i7j8",
   "metadata": {},
   "source": [
    "## Part 5: Query Preprocessing\n",
    "\n",
    "Good retrieval starts before you even search. Query preprocessing can significantly improve results by:\n",
    "- Normalizing formatting and whitespace\n",
    "- Expanding acronyms so both forms are searched\n",
    "- Classifying query intent to route to the best retrieval strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k9l0m1n2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class QueryPreprocessor:\n",
    "    \"\"\"Cleans and expands queries for better retrieval.\"\"\"\n",
    "    \n",
    "    def __init__(self, acronym_map=None):\n",
    "        self.acronym_map = acronym_map or {\n",
    "            \"RAG\": \"Retrieval-Augmented Generation\",\n",
    "            \"LLM\": \"Large Language Model\",\n",
    "            \"NLP\": \"Natural Language Processing\",\n",
    "            \"HNSW\": \"Hierarchical Navigable Small World\",\n",
    "        }\n",
    "    \n",
    "    def preprocess(self, query):\n",
    "        # Normalize whitespace\n",
    "        query = re.sub(r'\\s+', ' ', query.strip())\n",
    "        return query\n",
    "    \n",
    "    def expand_acronyms(self, query):\n",
    "        expanded = query\n",
    "        for acronym, full in self.acronym_map.items():\n",
    "            if acronym in query.upper():\n",
    "                expanded = f\"{expanded} ({full})\"\n",
    "        return expanded\n",
    "    \n",
    "    def classify_intent(self, query):\n",
    "        q = query.lower()\n",
    "        if any(w in q for w in [\"compare\", \"vs\", \"difference\", \"versus\"]):\n",
    "            return \"comparison\"\n",
    "        elif any(w in q for w in [\"how to\", \"how do\", \"implement\", \"build\"]):\n",
    "            return \"how_to\"\n",
    "        elif any(w in q for w in [\"what is\", \"what are\", \"define\", \"explain\"]):\n",
    "            return \"factual\"\n",
    "        elif any(w in q for w in [\"why\", \"error\", \"fail\", \"wrong\"]):\n",
    "            return \"troubleshooting\"\n",
    "        return \"general\"\n",
    "\n",
    "# Test\n",
    "preprocessor = QueryPreprocessor()\n",
    "queries = [\n",
    "    \"What is RAG?\",\n",
    "    \"Compare HNSW vs flat index\",\n",
    "    \"How to implement re-ranking?\",\n",
    "    \"Why is my retrieval accuracy low?\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    expanded = preprocessor.expand_acronyms(q)\n",
    "    intent = preprocessor.classify_intent(q)\n",
    "    print(f\"  [{intent:<16}] {q}\")\n",
    "    if expanded != q:\n",
    "        print(f\"  {'':>18} -> {expanded}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o3p4q5r6",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Add Query Expansion\n",
    "\n",
    "Query expansion generates multiple variations of a query to improve recall. In production, you would use an LLM to generate semantically diverse variations. For this exercise, implement a simple rule-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7t8u9v0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_variations(query, n=3):\n",
    "    \"\"\"\n",
    "    Generate search query variations to improve recall.\n",
    "    In production: use an LLM to generate variations.\n",
    "    \"\"\"\n",
    "    # TODO: Generate n variations of the query\n",
    "    # Strategy: append different suffixes that capture different aspects\n",
    "    # Example: \"What is RAG?\" -> [\"What is RAG?\", \"RAG methodology\", \"RAG state of the art\"]\n",
    "    variations = [query]  # Always include original\n",
    "    \n",
    "    # TODO: Add n-1 more variations\n",
    "    # Hint: Try appending words like \"methodology\", \"comparison\", \"implementation\", \"overview\"\n",
    "    \n",
    "    return variations[:n+1]  # Original + n variations\n",
    "\n",
    "# Test\n",
    "variations = generate_query_variations(\"What is chunking in RAG?\", n=3)\n",
    "from tests import checks\n",
    "checks.check_lab_4_5(variations)\n",
    "print(\"Query variations:\")\n",
    "for i, v in enumerate(variations):\n",
    "    print(f\"  {i+1}. {v}\")\n",
    "print(\"Query expansion working!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w1x2y3z4",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "1. **Hybrid Search**: When would pure BM25 outperform pure semantic search? Give a specific example query.\n",
    "2. **Re-ranking Cost**: If you have 1000 queries/day and retrieve 50 candidates each, how many cross-encoder inferences per day? Is this feasible?\n",
    "3. **RRF Parameter k**: What happens if k=1 (very small)? What about k=10000 (very large)? How does it affect the fusion?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6c7d8",
   "metadata": {},
   "source": [
    "*Your answers here:*\n",
    "1. ...\n",
    "2. ...\n",
    "3. ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}