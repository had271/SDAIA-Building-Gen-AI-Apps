{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4e5",
   "metadata": {},
   "source": [
    "# Lab 3: Building the RAG Service\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will:\n",
    "- Configure a ChromaDB vector store with HNSW indexing\n",
    "- Implement a VectorStoreManager for batch ingestion and search\n",
    "- Build an end-to-end RAGService that orchestrates Retrieve \u2192 Augment \u2192 Generate\n",
    "- Test your RAG system with real queries\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install chromadb numpy -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4c5",
   "metadata": {},
   "source": [
    "## Part 1: Vector Store Setup\n",
    "\n",
    "**ChromaDB** is an open-source embedding database that makes it easy to store, search, and retrieve vector embeddings. Under the hood, it uses **HNSW (Hierarchical Navigable Small World)** as its approximate nearest neighbor (ANN) index.\n",
    "\n",
    "Key HNSW parameters:\n",
    "- `hnsw:space` \u2014 Distance metric (`cosine`, `l2`, `ip`). We use `cosine` for text embeddings.\n",
    "- `hnsw:construction_ef` \u2014 Controls index build quality. Higher = better recall, slower build.\n",
    "- `hnsw:search_ef` \u2014 Controls search quality. Higher = better recall, slower search.\n",
    "- `hnsw:M` \u2014 Number of bi-directional links per node. Higher = better recall, more memory.\n",
    "\n",
    "We will create a `VectorStoreManager` class that wraps ChromaDB to provide a clean interface for document ingestion and search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e2f3a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "class VectorStoreManager:\n",
    "    \"\"\"Manages ChromaDB with HNSW indexing.\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name=\"research_papers\"):\n",
    "        # Use in-memory client for this lab\n",
    "        self.client = chromadb.Client()\n",
    "        \n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\n",
    "                \"hnsw:space\": \"cosine\",\n",
    "                \"hnsw:construction_ef\": 200,\n",
    "                \"hnsw:search_ef\": 100,\n",
    "                \"hnsw:M\": 16,\n",
    "            }\n",
    "        )\n",
    "        print(f\"Collection '{collection_name}' ready (HNSW cosine)\")\n",
    "    \n",
    "    def add_documents(self, documents, batch_size=100):\n",
    "        \"\"\"Add documents with embeddings in batches.\"\"\"\n",
    "        total = 0\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i:i+batch_size]\n",
    "            ids, embeddings, metadatas, texts = [], [], [], []\n",
    "            \n",
    "            for doc in batch:\n",
    "                content_hash = hashlib.md5(doc['text'].encode()).hexdigest()\n",
    "                ids.append(f\"doc_{content_hash}\")\n",
    "                embeddings.append(doc['embedding'])\n",
    "                metadatas.append(doc.get('metadata', {}))\n",
    "                texts.append(doc['text'])\n",
    "            \n",
    "            self.collection.add(\n",
    "                embeddings=embeddings,\n",
    "                documents=texts,\n",
    "                metadatas=metadatas,\n",
    "                ids=ids\n",
    "            )\n",
    "            total += len(batch)\n",
    "        \n",
    "        print(f\"Added {total} documents. Collection size: {self.collection.count()}\")\n",
    "        return total\n",
    "    \n",
    "    def search(self, query_embedding, n_results=5, filter_conditions=None):\n",
    "        \"\"\"Search for similar documents.\"\"\"\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n_results,\n",
    "            where=filter_conditions,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "        \n",
    "        formatted = []\n",
    "        for i in range(len(results[\"documents\"][0])):\n",
    "            formatted.append({\n",
    "                \"text\": results[\"documents\"][0][i],\n",
    "                \"metadata\": results[\"metadatas\"][0][i],\n",
    "                \"score\": 1 - results[\"distances\"][0][i]  # distance -> similarity\n",
    "            })\n",
    "        return formatted\n",
    "    \n",
    "    def get_stats(self):\n",
    "        return {\"count\": self.collection.count()}\n",
    "\n",
    "# Initialize\n",
    "store = VectorStoreManager()\n",
    "print(f\"Stats: {store.get_stats()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4a5",
   "metadata": {},
   "source": [
    "## Part 2: Populate with Sample Data\n",
    "\n",
    "We will create simulated research paper chunks with synthetic embeddings. In a production system, these embeddings would come from your ingestion and embedding pipeline (e.g., the `EmbeddingGenerator` from Lab 2).\n",
    "\n",
    "To make the simulation meaningful, we inject a topic-specific signal into each embedding so that semantically related chunks are closer together in vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c2d3e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated research paper chunks with pre-computed embeddings\n",
    "# In production, these would come from your ingestion + embedding pipeline\n",
    "np.random.seed(42)\n",
    "DIM = 384  # Using smaller dimension for demo\n",
    "\n",
    "sample_chunks = [\n",
    "    {\"text\": \"RAG combines retrieval with generation to ground LLM responses in external knowledge. This reduces hallucinations and enables citation of sources.\", \n",
    "     \"metadata\": {\"title\": \"RAG Survey\", \"section\": \"introduction\", \"chunk_id\": 0}},\n",
    "    {\"text\": \"The transformer architecture uses self-attention mechanisms to process all positions in a sequence simultaneously, enabling massive parallelization during training.\",\n",
    "     \"metadata\": {\"title\": \"Attention Is All You Need\", \"section\": \"architecture\", \"chunk_id\": 0}},\n",
    "    {\"text\": \"HNSW (Hierarchical Navigable Small World) is an approximate nearest neighbor algorithm that builds a multi-layered graph for fast vector search with high recall.\",\n",
    "     \"metadata\": {\"title\": \"HNSW Paper\", \"section\": \"algorithm\", \"chunk_id\": 0}},\n",
    "    {\"text\": \"Cosine similarity measures the angle between two vectors, making it robust to document length variation. It is the standard metric for text embeddings.\",\n",
    "     \"metadata\": {\"title\": \"Vector Search Guide\", \"section\": \"metrics\", \"chunk_id\": 0}},\n",
    "    {\"text\": \"Fine-tuning changes model weights through additional training, while RAG keeps the model frozen and provides knowledge through retrieval. RAG is more cost-effective for dynamic knowledge.\",\n",
    "     \"metadata\": {\"title\": \"RAG vs Fine-tuning\", \"section\": \"comparison\", \"chunk_id\": 0}},\n",
    "    {\"text\": \"Chunking is the process of splitting documents into smaller pieces for embedding. The optimal chunk size balances context preservation with retrieval precision.\",\n",
    "     \"metadata\": {\"title\": \"Chunking Strategies\", \"section\": \"fundamentals\", \"chunk_id\": 0}},\n",
    "    {\"text\": \"Cross-encoder re-ranking takes query-document pairs as input and produces a relevance score. It is more accurate than bi-encoders but slower.\",\n",
    "     \"metadata\": {\"title\": \"Re-ranking Survey\", \"section\": \"methods\", \"chunk_id\": 0}},\n",
    "    {\"text\": \"BM25 is a probabilistic ranking function based on term frequency. It excels at exact keyword matching, complementing semantic search in hybrid systems.\",\n",
    "     \"metadata\": {\"title\": \"Hybrid Search\", \"section\": \"bm25\", \"chunk_id\": 0}},\n",
    "    {\"text\": \"Vector quantization reduces memory by storing vectors with lower precision. Scalar quantization (SQ8) achieves 4x compression with minimal recall loss.\",\n",
    "     \"metadata\": {\"title\": \"Scaling Vectors\", \"section\": \"quantization\", \"chunk_id\": 0}},\n",
    "    {\"text\": \"The evaluation of RAG systems requires both retrieval metrics (Hit Rate, MRR) and generation metrics (faithfulness, relevance). DeepEval provides automated LLM-as-Judge evaluation.\",\n",
    "     \"metadata\": {\"title\": \"RAG Evaluation\", \"section\": \"metrics\", \"chunk_id\": 0}},\n",
    "]\n",
    "\n",
    "# Generate embeddings that cluster by topic (simulated)\n",
    "# In production, use your EmbeddingGenerator from Lab 2\n",
    "for i, chunk in enumerate(sample_chunks):\n",
    "    # Create a base vector + topic-specific signal\n",
    "    base = np.random.randn(DIM) * 0.1\n",
    "    # Add topic signal so similar chunks are closer\n",
    "    if \"RAG\" in chunk[\"text\"] or \"retrieval\" in chunk[\"text\"].lower():\n",
    "        base[:50] += 0.5  # RAG topic signal\n",
    "    if \"vector\" in chunk[\"text\"].lower() or \"embedding\" in chunk[\"text\"].lower():\n",
    "        base[50:100] += 0.5  # Vector topic signal\n",
    "    chunk[\"embedding\"] = (base / np.linalg.norm(base)).tolist()\n",
    "\n",
    "# Add to vector store\n",
    "store.add_documents(sample_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b3c4d5e6",
   "metadata": {},
   "source": [
    "## Part 3: Search Testing\n",
    "\n",
    "Let's test that our vector store returns relevant results. We create a query embedding with a RAG topic signal and verify that the top results are indeed about RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a8b9c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query embedding (simulated)\n",
    "query = \"What is RAG and how does it work?\"\n",
    "# Query embedding with RAG topic signal\n",
    "query_emb = np.random.randn(DIM) * 0.1\n",
    "query_emb[:50] += 0.5  # RAG topic signal\n",
    "query_emb = (query_emb / np.linalg.norm(query_emb)).tolist()\n",
    "\n",
    "results = store.search(query_emb, n_results=3)\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Top {len(results)} results:\")\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"\\n  [{i+1}] Score: {r['score']:.4f}\")\n",
    "    print(f\"      Title: {r['metadata'].get('title', 'N/A')}\")\n",
    "    print(f\"      Text: {r['text'][:120]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f3a4b5c6",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Metadata Filtering\n",
    "\n",
    "ChromaDB supports metadata filtering via the `where` parameter. This allows you to combine vector similarity search with structured filters -- for example, searching only within a specific section or document title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3f4a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Search for chunks specifically about \"architecture\" section\n",
    "# Use the filter_conditions parameter with: {\"section\": \"architecture\"}\n",
    "\n",
    "filtered_results = None  # Replace with: store.search(query_emb, n_results=5, filter_conditions={\"section\": \"architecture\"})\n",
    "\n",
    "assert filtered_results is not None, \"Call store.search with filter_conditions\"\n",
    "assert len(filtered_results) > 0, \"Should find at least one result\"\n",
    "assert all(r['metadata']['section'] == 'architecture' for r in filtered_results), \\\n",
    "    \"All results should be from 'architecture' section\"\n",
    "print(f\"Found {len(filtered_results)} results in 'architecture' section\")\n",
    "for r in filtered_results:\n",
    "    print(f\"  {r['metadata']['title']}: {r['text'][:80]}...\")\n",
    "print(\"Metadata filtering working!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3e4f5a6",
   "metadata": {},
   "source": [
    "## Part 4: The RAG Service\n",
    "\n",
    "Now we build the complete RAG pipeline that orchestrates three stages:\n",
    "\n",
    "1. **Retrieve** -- Embed the user query and search the vector store for relevant chunks.\n",
    "2. **Augment** -- Format the retrieved chunks into a context string and build a grounded prompt.\n",
    "3. **Generate** -- Send the augmented prompt to the LLM to produce a cited answer.\n",
    "\n",
    "In this lab we simulate embedding and generation. In production, you would replace `_embed_query` and `_generate` with calls to OpenAI, Anthropic, or another provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGService:\n",
    "    \"\"\"Orchestrates Retrieve -> Augment -> Generate.\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store):\n",
    "        self.vector_store = vector_store\n",
    "        self.embed_dim = DIM\n",
    "    \n",
    "    def _embed_query(self, query: str) -> list:\n",
    "        \"\"\"Generate query embedding. In production: call OpenAI API.\"\"\"\n",
    "        # Simulated embedding based on keywords\n",
    "        emb = np.random.randn(self.embed_dim) * 0.1\n",
    "        query_lower = query.lower()\n",
    "        if any(w in query_lower for w in [\"rag\", \"retrieval\", \"generation\"]):\n",
    "            emb[:50] += 0.5\n",
    "        if any(w in query_lower for w in [\"vector\", \"embedding\", \"cosine\"]):\n",
    "            emb[50:100] += 0.5\n",
    "        if any(w in query_lower for w in [\"chunk\", \"split\"]):\n",
    "            emb[100:150] += 0.5\n",
    "        return (emb / np.linalg.norm(emb)).tolist()\n",
    "    \n",
    "    def _build_context(self, results: list) -> str:\n",
    "        \"\"\"Format retrieved chunks into context string.\"\"\"\n",
    "        parts = []\n",
    "        for i, doc in enumerate(results, 1):\n",
    "            title = doc['metadata'].get('title', 'Unknown')\n",
    "            parts.append(f\"[{i}] (Source: {title})\\n{doc['text']}\")\n",
    "        return \"\\n\\n---\\n\\n\".join(parts)\n",
    "    \n",
    "    def _build_prompt(self, query: str, context: str) -> str:\n",
    "        \"\"\"Build the full prompt for the LLM.\"\"\"\n",
    "        system = \"\"\"You are a Research Assistant. Answer using ONLY the provided context.\n",
    "Cite sources using [N] format. If the answer is not in the context, say so.\"\"\"\n",
    "        \n",
    "        return f\"\"\"SYSTEM: {system}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    \n",
    "    def _generate(self, prompt: str) -> str:\n",
    "        \"\"\"Generate answer. In production: call OpenAI/Claude API.\"\"\"\n",
    "        # Simulated generation - extracts key sentences from context\n",
    "        lines = prompt.split(\"\\n\")\n",
    "        context_lines = [l for l in lines if l.startswith(\"[\")]\n",
    "        if context_lines:\n",
    "            return f\"Based on the retrieved sources: {context_lines[0][:200]}... [1]\"\n",
    "        return \"I don't have enough information to answer this question.\"\n",
    "    \n",
    "    def answer(self, query: str, top_k: int = 3) -> dict:\n",
    "        \"\"\"Full RAG pipeline: Retrieve -> Augment -> Generate.\"\"\"\n",
    "        # 1. RETRIEVE\n",
    "        query_embedding = self._embed_query(query)\n",
    "        results = self.vector_store.search(query_embedding, n_results=top_k)\n",
    "        \n",
    "        if not results:\n",
    "            return {\"answer\": \"No relevant information found.\", \"sources\": []}\n",
    "        \n",
    "        # 2. AUGMENT\n",
    "        context = self._build_context(results)\n",
    "        prompt = self._build_prompt(query, context)\n",
    "        \n",
    "        # 3. GENERATE\n",
    "        answer = self._generate(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": [r['metadata'] for r in results],\n",
    "            \"scores\": [r['score'] for r in results],\n",
    "            \"prompt_preview\": prompt[:500] + \"...\"\n",
    "        }\n",
    "\n",
    "# Test the complete pipeline\n",
    "rag = RAGService(store)\n",
    "response = rag.answer(\"What is RAG and why is it useful?\")\n",
    "\n",
    "print(f\"Answer: {response['answer']}\")\n",
    "print(f\"\\nSources:\")\n",
    "for s in response['sources']:\n",
    "    print(f\"  - {s.get('title', 'N/A')} ({s.get('section', 'N/A')})\")\n",
    "print(f\"\\nRetrieval scores: {[f'{s:.3f}' for s in response['scores']]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6e7",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Add Score Threshold\n",
    "\n",
    "In production, not all retrieved results are useful. Low-scoring results can introduce noise into the context and degrade generation quality. Implement a `min_score` threshold to filter out weak matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b0c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedRAGService(RAGService):\n",
    "    \"\"\"RAG Service with minimum score threshold.\"\"\"\n",
    "    \n",
    "    def answer(self, query: str, top_k: int = 3, min_score: float = 0.0) -> dict:\n",
    "        # TODO: Override the answer method to filter out results below min_score\n",
    "        # Steps:\n",
    "        # 1. Call self._embed_query(query)\n",
    "        # 2. Call self.vector_store.search(embedding, n_results=top_k)\n",
    "        # 3. Filter results where score >= min_score\n",
    "        # 4. If no results pass the threshold, return a \"no information\" message\n",
    "        # 5. Build context and generate as before\n",
    "        pass\n",
    "\n",
    "# Test\n",
    "improved_rag = ImprovedRAGService(store)\n",
    "result = improved_rag.answer(\"What is RAG?\", min_score=0.5)\n",
    "from tests import checks\n",
    "checks.check_lab_3_4(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e4f5a6b7",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Deduplication Check\n",
    "\n",
    "When ingesting documents at scale, you may encounter duplicates. Let's verify how ChromaDB handles re-adding documents that already exist in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Verify that adding the same documents again doesn't create duplicates\n",
    "initial_count = store.get_stats()['count']\n",
    "\n",
    "# Re-add the same documents\n",
    "store.add_documents(sample_chunks[:3])\n",
    "final_count = store.get_stats()['count']\n",
    "from tests import checks\n",
    "checks.check_lab_3_4_dedup(initial_count, final_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4d5e6f7",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "1. **HNSW Parameters**: What would happen if you set `hnsw:M` to 2 instead of 16? What about 128?\n",
    "2. **Score Interpretation**: A result comes back with score 0.72. Is this \"good enough\"? What factors determine the threshold?\n",
    "3. **Production Gap**: What are 3 things missing from our RAGService that a production system would need?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5c6d7e8",
   "metadata": {},
   "source": [
    "*Your answers here:*\n",
    "1. ...\n",
    "2. ...\n",
    "3. ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}