{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# Lab 1: Document Ingestion Pipeline\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will:\n",
    "- Evaluate document ingestion frameworks and their trade-offs\n",
    "- Implement a dispatcher pattern for multi-format document handling\n",
    "- Build a text cleaning pipeline for PDF and web content\n",
    "- Create a standardized Document data model for your RAG system\n",
    "\n",
    "## Setup\n",
    "Run the cell below to install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install beautifulsoup4 requests chardet -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part1-intro",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Framework Selection\n",
    "\n",
    "Before writing any code, the most important decision in a RAG ingestion pipeline is **choosing the right document processing framework**. Each framework has distinct strengths depending on your document types, accuracy requirements, and deployment constraints.\n",
    "\n",
    "Here are the four major frameworks we'll compare:\n",
    "\n",
    "| Framework | Approach | Key Strength |\n",
    "|-----------|----------|-------------|\n",
    "| **Docling** | Local ML models for layout analysis | Best table extraction accuracy (97.9%) |\n",
    "| **Unstructured** | Multi-format connectors + partitioning | Widest format support, enterprise connectors |\n",
    "| **LlamaParse** | Cloud API with LLM-powered parsing | LLM-optimized markdown output |\n",
    "| **PyMuPDF** | Direct PDF rendering library | Fastest raw text extraction |\n",
    "\n",
    "Let's build a comparison matrix to see them side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-part1-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "frameworks = {\n",
    "    \"Framework\": [\"Docling\", \"Unstructured\", \"LlamaParse\", \"PyMuPDF\"],\n",
    "    \"Table Accuracy\": [\"97.9%\", \"~75%\", \"High\", \"Basic\"],\n",
    "    \"Speed\": [\"~6s/page\", \"Variable\", \"API latency\", \"<1ms/page\"],\n",
    "    \"Deployment\": [\"Local (GPU/CPU)\", \"Local / API\", \"Cloud API\", \"Local\"],\n",
    "    \"License\": [\"MIT\", \"Apache 2.0\", \"Commercial\", \"AGPL\"],\n",
    "    \"Best For\": [\"Research papers\", \"Enterprise multi-format\", \"LLM-optimized output\", \"High-throughput simple PDFs\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(frameworks)\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-exercise1-intro",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Choose Your Framework\n",
    "\n",
    "Given the requirements below, which framework would you choose? Fill in your answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-exercise1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: For each scenario, assign the best framework name\n",
    "# Choices: \"Docling\", \"Unstructured\", \"LlamaParse\", \"PyMuPDF\"\n",
    "\n",
    "scenario_1 = \"\"  # Processing 50,000 simple PDF invoices as fast as possible\n",
    "scenario_2 = \"\"  # Extracting tables from scientific papers with 2-column layouts\n",
    "scenario_3 = \"\"  # Building a connector for emails, PowerPoints, and PDFs\n",
    "scenario_4 = \"\"  # Data must stay on-premise, complex table extraction needed\n",
    "\n",
    "# Validation\n",
    "from tests import checks\n",
    "checks.check_lab_1_1(scenario_1, scenario_2, scenario_3, scenario_4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part2-intro",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: The Document Model\n",
    "\n",
    "Every ingestion pipeline needs a **standardized data model** \u2014 a common structure that all extractors produce, regardless of the source format. This is critical because:\n",
    "\n",
    "1. **Downstream consistency**: Chunkers, embedders, and retrievers all expect the same shape of data.\n",
    "2. **Metadata tracking**: We need to know where content came from, when it was ingested, and what type it is.\n",
    "3. **Testability**: A clear contract makes it easy to validate extractor output.\n",
    "\n",
    "Let's define our `Document` dataclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-part2-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"Standardized representation of an ingested document.\"\"\"\n",
    "    content: str\n",
    "    source: str\n",
    "    title: Optional[str] = None\n",
    "    doc_type: str = \"unknown\"\n",
    "    author: Optional[str] = None\n",
    "    ingested_at: str = field(\n",
    "        default_factory=lambda: datetime.now(timezone.utc).isoformat()\n",
    "    )\n",
    "    word_count: int = 0\n",
    "    extra_metadata: Dict = field(default_factory=dict)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.content and self.word_count == 0:\n",
    "            self.word_count = len(self.content.split())\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            \"content\": self.content,\n",
    "            \"source\": self.source,\n",
    "            \"metadata\": {\n",
    "                \"title\": self.title,\n",
    "                \"type\": self.doc_type,\n",
    "                \"author\": self.author,\n",
    "                \"word_count\": self.word_count,\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Test it\n",
    "doc = Document(\n",
    "    content=\"Transformers use self-attention mechanisms to process sequences in parallel.\",\n",
    "    source=\"arxiv:1706.03762\",\n",
    "    title=\"Attention Is All You Need\",\n",
    "    doc_type=\"pdf\"\n",
    ")\n",
    "print(f\"Title: {doc.title}\")\n",
    "print(f\"Word count: {doc.word_count}\")\n",
    "print(f\"Dict keys: {list(doc.to_dict().keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part3-intro",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Building Extractors\n",
    "\n",
    "Now let's build our first real extractor \u2014 a **web page extractor** using BeautifulSoup. The key challenges in web extraction are:\n",
    "\n",
    "- **Boilerplate removal**: Navigation, footers, ads, and scripts are noise.\n",
    "- **Content detection**: Finding the `<main>` or `<article>` tag where the real content lives.\n",
    "- **Graceful degradation**: Not every page has clean semantic HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-part3-web-extractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_web_page(url: str) -> Document:\n",
    "    \"\"\"Extract clean text content from a web page.\"\"\"\n",
    "    resp = requests.get(\n",
    "        url,\n",
    "        headers={\"User-Agent\": \"ResearchAssistant/1.0\"},\n",
    "        timeout=10\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    \n",
    "    # Remove boilerplate elements\n",
    "    for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"form\"]):\n",
    "        tag.decompose()\n",
    "    \n",
    "    # Find main content area\n",
    "    content = soup.find(\"main\") or soup.find(\"article\") or soup.body\n",
    "    text = content.get_text(separator=\"\\n\", strip=True) if content else \"\"\n",
    "    title = soup.title.string if soup.title else url\n",
    "    \n",
    "    return Document(\n",
    "        content=text,\n",
    "        source=url,\n",
    "        title=title,\n",
    "        doc_type=\"web\"\n",
    "    )\n",
    "\n",
    "# Test with a real page\n",
    "try:\n",
    "    doc = extract_web_page(\"https://en.wikipedia.org/wiki/Retrieval-augmented_generation\")\n",
    "    print(f\"Title: {doc.title}\")\n",
    "    print(f\"Word count: {doc.word_count}\")\n",
    "    print(f\"Content preview: {doc.content[:300]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not fetch page (network issue): {e}\")\n",
    "    print(\"This is expected in offline environments.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part4-intro",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Text Cleaning Pipeline\n",
    "\n",
    "Raw extracted text is almost never ready for embedding. Common issues include:\n",
    "\n",
    "- **Encoding artifacts**: Smart quotes, em-dashes, and other Unicode oddities\n",
    "- **Excessive whitespace**: Triple newlines, tab characters, trailing spaces\n",
    "- **PDF artifacts**: Page numbers (\"Page 3 of 10\"), hyphenated line breaks\n",
    "\n",
    "Let's build a cleaning pipeline that handles all of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-part4-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Master cleaning function for extracted text.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Fix common encoding artifacts\n",
    "    text = text.replace(\"\\u2019\", \"'\").replace(\"\\u201c\", '\"').replace(\"\\u201d\", '\"')\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    \n",
    "    # 2. Normalize whitespace\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)  # Max 2 consecutive newlines\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)      # Collapse spaces/tabs\n",
    "    \n",
    "    # 3. Remove common PDF artifacts\n",
    "    text = re.sub(r\"Page \\d+ of \\d+\", \"\", text)\n",
    "    text = re.sub(r\"-\\n(\\w)\", r\"\\1\", text)  # Fix hyphenated line breaks\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Test\n",
    "dirty_text = \"\"\"This is   a   messy     text.\n",
    "\n",
    "\n",
    "Page 3 of 10\n",
    "\n",
    "It has    extra     spaces and    too many\n",
    "\n",
    "\n",
    "newlines.    The end-\n",
    "ing of words gets broken.\"\"\"\n",
    "\n",
    "cleaned = clean_text(dirty_text)\n",
    "print(\"BEFORE:\")\n",
    "print(repr(dirty_text[:200]))\n",
    "print(\"\\nAFTER:\")\n",
    "print(repr(cleaned[:200]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-exercise4-intro",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Extend the Cleaner\n",
    "\n",
    "The base `clean_text` function handles the most common issues, but real-world data has more noise. Add three new cleaning rules to handle emails, URLs, and standalone page numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-exercise4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_extended(text: str) -> str:\n",
    "    \"\"\"Extended cleaning with additional rules.\"\"\"\n",
    "    # Start with base cleaning\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # TODO: Add a regex to remove email addresses from the text\n",
    "    # Hint: Use re.sub with a pattern like r'\\S+@\\S+\\.\\S+'\n",
    "    \n",
    "    # TODO: Add a regex to remove URLs (http:// or https://)\n",
    "    # Hint: Use re.sub with a pattern like r'https?://\\S+'\n",
    "    \n",
    "    # TODO: Remove lines that are just numbers (e.g., page numbers)\n",
    "    # Hint: Use re.sub with r'^\\d+$' and re.MULTILINE flag\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Test your implementation\n",
    "test_text = \"\"\"\n",
    "Contact us at info@example.com for more details.\n",
    "Visit https://www.example.com/research for the full paper.\n",
    "42\n",
    "This is the actual content we want to keep.\n",
    "\"\"\"\n",
    "\n",
    "result = clean_text_extended(test_text)\n",
    "print(f\"Result: {repr(result)}\")\n",
    "\n",
    "# Validation\n",
    "from tests import checks\n",
    "checks.check_lab_1_4(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part5-intro",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: The Dispatcher Pattern\n",
    "\n",
    "In a production RAG system, documents arrive in many formats. Rather than writing `if/elif` chains everywhere, we use a **dispatcher** \u2014 a single entry point that routes to the correct extractor based on the source type.\n",
    "\n",
    "This pattern provides:\n",
    "- **Single entry point**: Callers don't need to know which extractor to use.\n",
    "- **Easy extensibility**: Adding a new format means adding one branch.\n",
    "- **Consistent output**: Every extractor returns the same `Document` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-part5-dispatcher",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def extract_document(source: str) -> Document:\n",
    "    \"\"\"Route to the correct extractor based on source type.\"\"\"\n",
    "    if source.startswith((\"http://\", \"https://\")):\n",
    "        return extract_web_page(source)\n",
    "    \n",
    "    ext = Path(source).suffix.lower()\n",
    "    \n",
    "    if ext in (\".md\", \".txt\"):\n",
    "        text = Path(source).read_text(encoding=\"utf-8\")\n",
    "        return Document(content=text, source=source, doc_type=\"text\")\n",
    "    elif ext == \".pdf\":\n",
    "        # In production: use Docling here\n",
    "        # from docling.document_converter import DocumentConverter\n",
    "        raise NotImplementedError(\n",
    "            \"PDF extraction requires Docling. \"\n",
    "            \"Install with: pip install docling\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported format: {ext}\")\n",
    "\n",
    "# Demonstrate the dispatcher\n",
    "print(\"Dispatcher routing:\")\n",
    "for source in [\"https://example.com/paper\", \"report.pdf\", \"notes.md\", \"data.csv\"]:\n",
    "    try:\n",
    "        print(f\"  {source} -> \", end=\"\")\n",
    "        # Don't actually call - just show routing logic\n",
    "        if source.startswith(\"http\"):\n",
    "            print(\"extract_web_page()\")\n",
    "        elif source.endswith(\".pdf\"):\n",
    "            print(\"extract_pdf() [Docling]\")\n",
    "        elif source.endswith((\".md\", \".txt\")):\n",
    "            print(\"extract_text_file()\")\n",
    "        else:\n",
    "            print(f\"ValueError: Unsupported format\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-exercise5-intro",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Add Markdown Support\n",
    "\n",
    "Implement a dedicated markdown extractor that goes beyond plain text reading. It should:\n",
    "1. Read the file content\n",
    "2. Extract the title from the first `#` heading (if present)\n",
    "3. Return a `Document` with `doc_type=\"markdown\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-exercise5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a markdown extractor and integrate it into the dispatcher\n",
    "def extract_markdown(file_path: str) -> Document:\n",
    "    \"\"\"Extract content from a markdown file with basic metadata.\"\"\"\n",
    "    # TODO: Read the file content\n",
    "    # TODO: Extract the title from the first # heading (if present)\n",
    "    # TODO: Return a Document object with doc_type=\"markdown\"\n",
    "    \n",
    "    # Hint: Use Path(file_path).read_text() and check lines starting with \"# \"\n",
    "    pass\n",
    "\n",
    "# Test with an inline markdown string (simulating a file)\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "sample_md = \"\"\"# My Research Notes\n",
    "\n",
    "## Introduction\n",
    "This is a sample markdown document about RAG systems.\n",
    "\n",
    "## Key Findings\n",
    "- Chunking strategy matters most\n",
    "- Overlap prevents context loss\n",
    "\"\"\"\n",
    "\n",
    "# Write to temp file for testing\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False, encoding='utf-8') as f:\n",
    "    f.write(sample_md)\n",
    "    temp_path = f.name\n",
    "\n",
    "try:\n",
    "    doc = extract_markdown(temp_path)\n",
    "    from tests import checks\n",
    "    checks.check_lab_1_5(doc)\n",
    "finally:\n",
    "    os.unlink(temp_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-reflection",
   "metadata": {},
   "source": [
    "---\n",
    "## Reflection Questions\n",
    "\n",
    "1. **Trade-offs**: Why might you use PyMuPDF for a first pass and Docling as a fallback? What metric would you use to decide when to escalate?\n",
    "2. **Data Model**: What additional metadata fields would you add to the Document class for a legal document processing system?\n",
    "3. **Cleaning**: What risks come with aggressive text cleaning? Can you think of a case where removing \"noise\" actually removes important information?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-reflection-answers",
   "metadata": {},
   "source": [
    "*Your answers here:*\n",
    "\n",
    "1. ...\n",
    "2. ...\n",
    "3. ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}