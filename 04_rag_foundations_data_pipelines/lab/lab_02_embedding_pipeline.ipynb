{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Lab 2: From Chunks to Vectors\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will:\n",
    "- Implement a modular chunking system with a base class and factory pattern\n",
    "- Compare fixed-size, recursive, and semantic chunking strategies\n",
    "- Generate embeddings at scale with batch processing and retry logic\n",
    "- Analyze chunk size distributions across strategies\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install langchain langchain-text-splitters sentence-transformers numpy matplotlib -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "## Part 1: The Chunker Base Class\n",
    "\n",
    "We start by defining a **base class** that all chunking strategies will inherit from. This gives us:\n",
    "\n",
    "- A consistent interface (`chunk_document`) across all strategies\n",
    "- Shared helper methods (`_create_chunk_dict`) to standardize chunk metadata\n",
    "- Easy extensibility: adding a new strategy means implementing one method\n",
    "\n",
    "This is the **Template Method** pattern in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class BaseChunker(ABC):\n",
    "    \"\"\"Abstract base class for all chunking strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "    \n",
    "    @abstractmethod\n",
    "    def chunk_document(self, text: str, metadata: dict = None) -> List[Dict[str, Any]]:\n",
    "        pass\n",
    "    \n",
    "    def _create_chunk_dict(self, text: str, metadata: dict, chunk_id: int) -> dict:\n",
    "        chunk_meta = metadata.copy() if metadata else {}\n",
    "        chunk_meta.update({\n",
    "            'chunk_id': chunk_id,\n",
    "            'char_length': len(text),\n",
    "            'chunker': self.__class__.__name__\n",
    "        })\n",
    "        return {'text': text.strip(), 'metadata': chunk_meta}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8",
   "metadata": {},
   "source": [
    "## Part 2: Fixed-Size Chunker\n",
    "\n",
    "The simplest strategy: split text into chunks of a fixed character count, with an overlap window to preserve context at boundaries. This is fast and predictable but can cut sentences mid-thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedSizeChunker(BaseChunker):\n",
    "    \"\"\"Splits text into fixed-size chunks with overlap.\"\"\"\n",
    "    \n",
    "    def chunk_document(self, text, metadata=None):\n",
    "        chunks = []\n",
    "        step = self.chunk_size - self.chunk_overlap\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = min(start + self.chunk_size, len(text))\n",
    "            chunk_text = text[start:end]\n",
    "            if chunk_text.strip():\n",
    "                chunks.append(self._create_chunk_dict(chunk_text, metadata, len(chunks)))\n",
    "            start += step\n",
    "        return chunks\n",
    "\n",
    "# Test\n",
    "sample = \"The transformer architecture uses self-attention. \" * 50\n",
    "chunker = FixedSizeChunker(chunk_size=200, chunk_overlap=30)\n",
    "chunks = chunker.chunk_document(sample, {\"source\": \"test\"})\n",
    "print(f\"Input length: {len(sample)} chars\")\n",
    "print(f\"Chunks created: {len(chunks)}\")\n",
    "print(f\"First chunk ({len(chunks[0]['text'])} chars): {chunks[0]['text'][:100]}...\")\n",
    "print(f\"Metadata: {chunks[0]['metadata']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0",
   "metadata": {},
   "source": [
    "## Part 3: Recursive Chunker\n",
    "\n",
    "The recursive strategy tries to split on natural boundaries first (paragraphs, then sentences, then words) before falling back to character-level splits. This produces more semantically coherent chunks.\n",
    "\n",
    "We wrap LangChain's `RecursiveCharacterTextSplitter` inside our `BaseChunker` interface for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "class RecursiveChunker(BaseChunker):\n",
    "    \"\"\"Chunks respecting paragraph and sentence boundaries.\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size=500, chunk_overlap=50, separators=None):\n",
    "        super().__init__(chunk_size, chunk_overlap)\n",
    "        self.separators = separators or [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        self._splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            separators=self.separators,\n",
    "        )\n",
    "    \n",
    "    def chunk_document(self, text, metadata=None):\n",
    "        text_chunks = self._splitter.split_text(text)\n",
    "        return [self._create_chunk_dict(t, metadata, i) for i, t in enumerate(text_chunks)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "## Part 4: Head-to-Head Comparison\n",
    "\n",
    "Now let's compare both strategies on a research paper-like text. We will look at:\n",
    "- Number of chunks produced\n",
    "- Size distribution across chunks\n",
    "- How well each strategy respects section boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Simulated research paper text with sections\n",
    "paper_text = \"\"\"ABSTRACT\n",
    "We present a novel approach to retrieval-augmented generation that improves accuracy by 15% over existing baselines. Our method combines semantic chunking with hybrid retrieval.\n",
    "\n",
    "1. INTRODUCTION\n",
    "Retrieval-Augmented Generation (RAG) has become pivotal in building knowledge-grounded AI systems. However, the quality of retrieval depends critically on how documents are chunked.\n",
    "\n",
    "1.1 Background\n",
    "Previous work used fixed-size chunks with arbitrary boundaries. This often cuts sentences mid-thought, destroying semantic coherence. We propose a document-aware method that respects natural boundaries.\n",
    "\n",
    "1.2 Motivation\n",
    "In production systems, chunking quality directly determines answer quality. Poor chunks lead to poor retrieval, which leads to hallucinated or incomplete answers.\n",
    "\n",
    "2. METHODOLOGY\n",
    "Our approach works in three stages: first, we parse the document structure; second, we identify semantic boundaries; third, we merge adjacent similar segments.\n",
    "\n",
    "2.1 Document Parsing\n",
    "We use a layout-aware parser to identify headers, paragraphs, and tables. Each structural element becomes a candidate chunk boundary.\n",
    "\n",
    "2.2 Boundary Detection\n",
    "Using sentence embeddings, we compute cosine distance between adjacent segments. Distances above the 90th percentile indicate topic shifts.\n",
    "\n",
    "3. RESULTS\n",
    "Our method produces chunks that are 23% more coherent than fixed-size baselines, as measured by intra-chunk semantic similarity. Retrieval accuracy improves by 15% on our benchmark.\n",
    "\n",
    "4. CONCLUSION\n",
    "Document-aware chunking is essential for production RAG systems. Future work will explore content-type specific strategies.\"\"\"\n",
    "\n",
    "metadata = {\"source\": \"sample_paper\", \"title\": \"RAG Chunking Study\"}\n",
    "\n",
    "# Compare strategies\n",
    "fixed = FixedSizeChunker(chunk_size=300, chunk_overlap=50)\n",
    "recursive = RecursiveChunker(chunk_size=300, chunk_overlap=50)\n",
    "\n",
    "fixed_chunks = fixed.chunk_document(paper_text, metadata)\n",
    "recursive_chunks = recursive.chunk_document(paper_text, metadata)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Chunk size distribution\n",
    "for ax, chunks, name, color in [\n",
    "    (axes[0], fixed_chunks, \"Fixed-Size\", \"#FF7A5C\"),\n",
    "    (axes[1], recursive_chunks, \"Recursive\", \"#00C9A7\")\n",
    "]:\n",
    "    sizes = [len(c['text']) for c in chunks]\n",
    "    ax.bar(range(len(sizes)), sizes, color=color, alpha=0.8)\n",
    "    ax.set_title(f\"{name} ({len(chunks)} chunks)\")\n",
    "    ax.set_xlabel(\"Chunk Index\")\n",
    "    ax.set_ylabel(\"Characters\")\n",
    "    ax.axhline(y=300, color='#1C355E', linestyle='--', label='Target size')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle(\"Chunk Size Distribution: Fixed vs Recursive\", fontsize=14, color='#1C355E')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comparison\n",
    "print(f\"\\n{'Metric':<25} {'Fixed-Size':>12} {'Recursive':>12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Number of chunks':<25} {len(fixed_chunks):>12} {len(recursive_chunks):>12}\")\n",
    "f_sizes = [len(c['text']) for c in fixed_chunks]\n",
    "r_sizes = [len(c['text']) for c in recursive_chunks]\n",
    "print(f\"{'Avg chunk size (chars)':<25} {np.mean(f_sizes):>12.0f} {np.mean(r_sizes):>12.0f}\")\n",
    "print(f\"{'Std deviation':<25} {np.std(f_sizes):>12.0f} {np.std(r_sizes):>12.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4",
   "metadata": {},
   "source": [
    "### Exercise 4.1: ChunkerFactory\n",
    "\n",
    "Implement the factory pattern to automatically select the right chunking strategy based on content type. Complete the `get_chunker` method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkerFactory:\n",
    "    \"\"\"Factory to create the right chunker for each content type.\"\"\"\n",
    "    \n",
    "    _strategies = {\n",
    "        'fixed': FixedSizeChunker,\n",
    "        'recursive': RecursiveChunker,\n",
    "    }\n",
    "    \n",
    "    _content_type_mapping = {\n",
    "        'research_paper': 'recursive',\n",
    "        'legal_document': 'recursive',\n",
    "        'code': 'fixed',\n",
    "        'chat_logs': 'fixed',\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def get_chunker(cls, content_type='research_paper', strategy=None, **kwargs):\n",
    "        # TODO: If strategy is None, look up the content_type in _content_type_mapping\n",
    "        # TODO: Get the chunker class from _strategies\n",
    "        # TODO: Return an instance created with **kwargs\n",
    "        pass\n",
    "\n",
    "# Validation\n",
    "chunker = ChunkerFactory.get_chunker('research_paper', chunk_size=500, chunk_overlap=100)\n",
    "chunker2 = ChunkerFactory.get_chunker('code', chunk_size=300)\n",
    "chunker3 = ChunkerFactory.get_chunker(strategy='fixed', chunk_size=200)\n",
    "\n",
    "from tests import checks\n",
    "checks.check_lab_2_4(chunker, chunker2, chunker3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6",
   "metadata": {},
   "source": [
    "## Part 5: Embedding Generation\n",
    "\n",
    "Once we have chunks, we need to convert them into vectors (embeddings). In production, this means:\n",
    "\n",
    "- **Batching**: Send multiple texts per API call to reduce overhead\n",
    "- **Retry logic**: Handle rate limits and transient failures with exponential backoff\n",
    "- **Cost tracking**: Monitor token usage to control costs\n",
    "\n",
    "Below is a simulated embedding generator that demonstrates these patterns. In production, you would replace the simulated calls with real OpenAI API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"Simulates embedding generation with batching and retry logic.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"text-embedding-3-small\", dimension=1536):\n",
    "        self.model_name = model_name\n",
    "        self.dimension = dimension\n",
    "        self.total_tokens = 0\n",
    "    \n",
    "    def _embed_batch(self, texts: list, attempt=1, max_retries=3) -> list:\n",
    "        \"\"\"Generate embeddings for a batch with retry logic.\"\"\"\n",
    "        try:\n",
    "            # Simulate API call (in production: call OpenAI API)\n",
    "            time.sleep(0.01 * len(texts))  # Simulate latency\n",
    "            \n",
    "            # Simulate occasional rate limit (10% chance)\n",
    "            if random.random() < 0.1 and attempt < max_retries:\n",
    "                raise Exception(\"Rate limit exceeded (simulated)\")\n",
    "            \n",
    "            # Generate fake embeddings (in production: real API response)\n",
    "            embeddings = [np.random.randn(self.dimension).tolist() for _ in texts]\n",
    "            self.total_tokens += sum(len(t.split()) * 1.3 for t in texts)  # Approximate\n",
    "            return embeddings\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries:\n",
    "                wait = 2 ** attempt  # Exponential backoff\n",
    "                print(f\"  Retry {attempt}/{max_retries} after {wait}s: {e}\")\n",
    "                time.sleep(wait * 0.01)  # Shortened for demo\n",
    "                return self._embed_batch(texts, attempt + 1, max_retries)\n",
    "            raise\n",
    "    \n",
    "    def generate_embeddings(self, texts: list, batch_size=100) -> list:\n",
    "        \"\"\"Process all texts in batches.\"\"\"\n",
    "        all_embeddings = []\n",
    "        total_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            batch_num = i // batch_size + 1\n",
    "            print(f\"  Processing batch {batch_num}/{total_batches} ({len(batch)} texts)...\")\n",
    "            embeddings = self._embed_batch(batch)\n",
    "            all_embeddings.extend(embeddings)\n",
    "        \n",
    "        print(f\"Generated {len(all_embeddings)} embeddings, ~{self.total_tokens:.0f} tokens used\")\n",
    "        return all_embeddings\n",
    "\n",
    "# Test with our chunks\n",
    "generator = EmbeddingGenerator()\n",
    "chunk_texts = [c['text'] for c in recursive_chunks]\n",
    "embeddings = generator.generate_embeddings(chunk_texts, batch_size=3)\n",
    "print(f\"Embedding dimension: {len(embeddings[0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Cost Calculator\n",
    "\n",
    "Implement a function to estimate embedding costs before committing to a full pipeline run. This is critical for budgeting in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_embedding_cost(num_chunks, avg_tokens_per_chunk, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"Estimate the cost of embedding a corpus.\"\"\"\n",
    "    # Pricing per 1M tokens (as of 2025)\n",
    "    pricing = {\n",
    "        \"text-embedding-3-small\": 0.02,\n",
    "        \"text-embedding-3-large\": 0.13,\n",
    "    }\n",
    "    \n",
    "    # TODO: Calculate total tokens\n",
    "    total_tokens = None  # num_chunks * avg_tokens_per_chunk\n",
    "    \n",
    "    # TODO: Calculate cost\n",
    "    cost = None  # (total_tokens / 1_000_000) * pricing[model]\n",
    "    \n",
    "    return {\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"cost_usd\": cost,\n",
    "        \"model\": model\n",
    "    }\n",
    "\n",
    "# Test: 10,000 chunks, average 150 tokens each\n",
    "result = estimate_embedding_cost(10_000, 150)\n",
    "from tests import checks\n",
    "checks.check_lab_2_5(result)\n",
    "\n",
    "# Compare models\n",
    "for model in [\"text-embedding-3-small\", \"text-embedding-3-large\"]:\n",
    "    r = estimate_embedding_cost(10_000, 150, model)\n",
    "    if r[\"cost_usd\"]:\n",
    "        print(f\"  {model}: ${r['cost_usd']:.4f}\")\n",
    "print(\"Cost calculator working!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8a9b0",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "1. **Chunking**: When would recursive chunking perform worse than fixed-size? Think about document types.\n",
    "2. **Overlap**: If you set overlap to 50% of chunk size, what problems might arise?\n",
    "3. **Cost**: Your corpus has 100,000 research papers averaging 20 pages. Estimate the embedding cost using text-embedding-3-small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b0c1",
   "metadata": {},
   "source": [
    "*Your answers here:*\n",
    "1. ...\n",
    "2. ...\n",
    "3. ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}