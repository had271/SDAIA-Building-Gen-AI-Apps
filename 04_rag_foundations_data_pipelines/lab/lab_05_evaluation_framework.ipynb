{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4e5",
   "metadata": {},
   "source": [
    "# Lab 5: Building the Evaluation Framework\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will:\n",
    "- Create a Golden Dataset for retrieval evaluation\n",
    "- Implement Hit Rate@K, MRR, Precision@K, and Recall@K metrics from scratch\n",
    "- Visualize the Precision-Recall trade-off\n",
    "- Understand LLM-as-Judge evaluation with DeepEval concepts\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6g7h8i9j0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install numpy matplotlib -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1l2m3n4o5",
   "metadata": {},
   "source": [
    "## Part 1: The Golden Dataset\n",
    "\n",
    "A **golden dataset** (also called a ground-truth dataset) is a hand-curated collection of query-document relevance pairs. It serves as the **single source of truth** for evaluating your retrieval pipeline.\n",
    "\n",
    "Why golden datasets are critical:\n",
    "- **Reproducibility**: Without fixed evaluation data, you cannot compare experiments meaningfully.\n",
    "- **Regression detection**: When you change chunking strategies, embedding models, or retrieval parameters, the golden dataset tells you if quality improved or degraded.\n",
    "- **Human alignment**: Domain experts label what \"relevant\" means, anchoring your metrics to real-world expectations.\n",
    "\n",
    "Each entry maps a **query** to the set of **document IDs** that a human has judged as relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6q7r8s9t0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Golden Dataset: hand-curated query -> relevant document pairs\n",
    "golden_dataset = [\n",
    "    {\n",
    "        \"query_id\": \"q1\",\n",
    "        \"query\": \"What is Retrieval-Augmented Generation?\",\n",
    "        \"relevant_doc_ids\": {\"doc_rag_intro_0\", \"doc_rag_overview_1\"},\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"q2\",\n",
    "        \"query\": \"Explain the transformer self-attention mechanism\",\n",
    "        \"relevant_doc_ids\": {\"doc_attention_0\", \"doc_attention_1\", \"doc_transformer_arch_0\"},\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"q3\",\n",
    "        \"query\": \"Compare BM25 with semantic search\",\n",
    "        \"relevant_doc_ids\": {\"doc_hybrid_search_0\", \"doc_bm25_explained_0\"},\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"q4\",\n",
    "        \"query\": \"How does HNSW indexing work?\",\n",
    "        \"relevant_doc_ids\": {\"doc_hnsw_algorithm_0\", \"doc_hnsw_params_0\"},\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"q5\",\n",
    "        \"query\": \"What chunking strategy is best for research papers?\",\n",
    "        \"relevant_doc_ids\": {\"doc_chunking_strategies_0\", \"doc_recursive_chunking_0\"},\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Golden dataset: {len(golden_dataset)} queries\")\n",
    "for item in golden_dataset:\n",
    "    print(f\"  {item['query_id']}: '{item['query'][:50]}...' -> {len(item['relevant_doc_ids'])} relevant docs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u1v2w3x4y5",
   "metadata": {},
   "source": [
    "## Part 2: Simulated Retrieval Results\n",
    "\n",
    "To evaluate our metrics, we need retrieval results to score against the golden dataset. Below we simulate what a retrieval system might return: an **ordered list** of document IDs for each query, ranked from most to least relevant (as the system sees it).\n",
    "\n",
    "Some retrievals are good (relevant docs ranked first), some are poor (relevant docs buried or missing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z6a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated retrieval results (what our system actually returned)\n",
    "# Each query -> ordered list of doc IDs retrieved (best first)\n",
    "simulated_retrievals = {\n",
    "    \"q1\": [\"doc_rag_intro_0\", \"doc_other_1\", \"doc_rag_overview_1\", \"doc_noise_2\", \"doc_noise_3\"],\n",
    "    \"q2\": [\"doc_unrelated_0\", \"doc_attention_1\", \"doc_noise_1\", \"doc_attention_0\", \"doc_noise_2\"],\n",
    "    \"q3\": [\"doc_hybrid_search_0\", \"doc_bm25_explained_0\", \"doc_noise_1\", \"doc_noise_2\", \"doc_noise_3\"],\n",
    "    \"q4\": [\"doc_noise_0\", \"doc_noise_1\", \"doc_hnsw_algorithm_0\", \"doc_noise_2\", \"doc_hnsw_params_0\"],\n",
    "    \"q5\": [\"doc_chunking_strategies_0\", \"doc_noise_0\", \"doc_noise_1\", \"doc_noise_2\", \"doc_noise_3\"],\n",
    "}\n",
    "\n",
    "print(\"Simulated retrieval results (top 5 per query):\")\n",
    "for qid, docs in simulated_retrievals.items():\n",
    "    query = next(g for g in golden_dataset if g['query_id'] == qid)\n",
    "    relevant = query['relevant_doc_ids']\n",
    "    hits = [(\"*\" if d in relevant else \" \") for d in docs]\n",
    "    print(f\"  {qid}: {' | '.join(f'{h}{d[:20]}' for h, d in zip(hits, docs))}\")\n",
    "print(\"\\n  (* = relevant document)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2g3h4i5",
   "metadata": {},
   "source": [
    "## Part 3: Implementing Retrieval Metrics\n",
    "\n",
    "We will implement four core retrieval metrics from scratch:\n",
    "\n",
    "| Metric | What it measures | Range |\n",
    "|--------|-----------------|-------|\n",
    "| **Hit Rate@K** | Did we find *at least one* relevant doc in top K? | 0-1 |\n",
    "| **MRR** (Mean Reciprocal Rank) | How early does the first relevant doc appear? | 0-1 |\n",
    "| **Precision@K** | What fraction of the top K results are relevant? | 0-1 |\n",
    "| **Recall@K** | What fraction of *all* relevant docs did we find in top K? | 0-1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j6k7l8m9n0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set\n",
    "\n",
    "def hit_rate_at_k(retrieved: List[List[str]], relevant: List[Set[str]], k: int) -> float:\n",
    "    \"\"\"Fraction of queries where at least one relevant doc is in top K.\"\"\"\n",
    "    hits = 0\n",
    "    for ret, rel in zip(retrieved, relevant):\n",
    "        if any(doc_id in rel for doc_id in ret[:k]):\n",
    "            hits += 1\n",
    "    return hits / len(retrieved)\n",
    "\n",
    "def mean_reciprocal_rank(retrieved: List[List[str]], relevant: List[Set[str]]) -> float:\n",
    "    \"\"\"Average of 1/rank of first relevant document.\"\"\"\n",
    "    rr_sum = 0.0\n",
    "    for ret, rel in zip(retrieved, relevant):\n",
    "        for rank, doc_id in enumerate(ret, 1):\n",
    "            if doc_id in rel:\n",
    "                rr_sum += 1.0 / rank\n",
    "                break\n",
    "    return rr_sum / len(retrieved)\n",
    "\n",
    "def precision_at_k(retrieved: List[List[str]], relevant: List[Set[str]], k: int) -> float:\n",
    "    \"\"\"Average fraction of relevant docs in top K.\"\"\"\n",
    "    precisions = []\n",
    "    for ret, rel in zip(retrieved, relevant):\n",
    "        top_k = ret[:k]\n",
    "        relevant_in_k = sum(1 for d in top_k if d in rel)\n",
    "        precisions.append(relevant_in_k / k)\n",
    "    return sum(precisions) / len(precisions)\n",
    "\n",
    "def recall_at_k(retrieved: List[List[str]], relevant: List[Set[str]], k: int) -> float:\n",
    "    \"\"\"Average fraction of all relevant docs found in top K.\"\"\"\n",
    "    recalls = []\n",
    "    for ret, rel in zip(retrieved, relevant):\n",
    "        if not rel:\n",
    "            recalls.append(0.0)\n",
    "            continue\n",
    "        top_k = ret[:k]\n",
    "        found = sum(1 for d in top_k if d in rel)\n",
    "        recalls.append(found / len(rel))\n",
    "    return sum(recalls) / len(recalls)\n",
    "\n",
    "# Prepare data\n",
    "all_retrieved = [simulated_retrievals[g['query_id']] for g in golden_dataset]\n",
    "all_relevant = [g['relevant_doc_ids'] for g in golden_dataset]\n",
    "\n",
    "# Calculate metrics at different K values\n",
    "print(\"Retrieval Metrics:\")\n",
    "print(f\"{'K':>3} {'Hit Rate':>10} {'MRR':>10} {'Precision':>10} {'Recall':>10}\")\n",
    "print(\"-\" * 45)\n",
    "for k in [1, 3, 5]:\n",
    "    hr = hit_rate_at_k(all_retrieved, all_relevant, k)\n",
    "    mrr = mean_reciprocal_rank(all_retrieved, all_relevant)\n",
    "    prec = precision_at_k(all_retrieved, all_relevant, k)\n",
    "    rec = recall_at_k(all_retrieved, all_relevant, k)\n",
    "    print(f\"{k:>3} {hr:>10.3f} {mrr:>10.3f} {prec:>10.3f} {rec:>10.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o1p2q3r4s5",
   "metadata": {},
   "source": [
    "## Part 4: Precision-Recall Visualization\n",
    "\n",
    "The **Precision-Recall trade-off** is fundamental: as you increase K (retrieve more documents), recall goes up (you find more relevant docs) but precision tends to go down (you also retrieve more noise). The sweet spot depends on your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t6u7v8w9x0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "k_values = list(range(1, 6))\n",
    "precisions = [precision_at_k(all_retrieved, all_relevant, k) for k in k_values]\n",
    "recalls = [recall_at_k(all_retrieved, all_relevant, k) for k in k_values]\n",
    "hit_rates = [hit_rate_at_k(all_retrieved, all_relevant, k) for k in k_values]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# Precision & Recall vs K\n",
    "ax1.plot(k_values, precisions, 'o-', color='#FF7A5C', linewidth=2, markersize=8, label='Precision@K')\n",
    "ax1.plot(k_values, recalls, 's-', color='#00C9A7', linewidth=2, markersize=8, label='Recall@K')\n",
    "ax1.plot(k_values, hit_rates, '^-', color='#9B8EC0', linewidth=2, markersize=8, label='Hit Rate@K')\n",
    "ax1.set_xlabel('K (number of results)', fontsize=12)\n",
    "ax1.set_ylabel('Score', fontsize=12)\n",
    "ax1.set_title('Metrics vs K', fontsize=14, color='#1C355E')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.set_ylim(0, 1.05)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Precision vs Recall curve\n",
    "ax2.plot(recalls, precisions, 'o-', color='#1C355E', linewidth=2, markersize=10)\n",
    "for i, k in enumerate(k_values):\n",
    "    ax2.annotate(f'K={k}', (recalls[i], precisions[i]), textcoords=\"offset points\",\n",
    "                xytext=(10, 5), fontsize=10, color='#FF7A5C')\n",
    "ax2.set_xlabel('Recall@K', fontsize=12)\n",
    "ax2.set_ylabel('Precision@K', fontsize=12)\n",
    "ax2.set_title('Precision-Recall Trade-off', fontsize=14, color='#1C355E')\n",
    "ax2.set_xlim(0, 1.05)\n",
    "ax2.set_ylim(0, 1.05)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y1z2a3b4c5",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Per-Query Analysis\n",
    "\n",
    "Aggregate metrics hide per-query performance. Complete the function below to analyze each query individually and identify weak spots in your retrieval pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8g9h0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_query_analysis(golden_dataset, simulated_retrievals, k=3):\n",
    "    \"\"\"Analyze each query individually to find weak spots.\"\"\"\n",
    "    results = []\n",
    "    for item in golden_dataset:\n",
    "        qid = item['query_id']\n",
    "        retrieved = simulated_retrievals[qid][:k]\n",
    "        relevant = item['relevant_doc_ids']\n",
    "        \n",
    "        # TODO: Calculate hit (True/False) - is any relevant doc in top K?\n",
    "        hit = None\n",
    "        \n",
    "        # TODO: Calculate precision for this query\n",
    "        prec = None  # relevant found in top k / k\n",
    "        \n",
    "        # TODO: Calculate recall for this query\n",
    "        rec = None  # relevant found in top k / total relevant\n",
    "        \n",
    "        # TODO: Find the rank of first relevant doc (0 if not found in top K)\n",
    "        first_relevant_rank = 0\n",
    "        for rank, doc_id in enumerate(retrieved, 1):\n",
    "            if doc_id in relevant:\n",
    "                first_relevant_rank = rank\n",
    "                break\n",
    "        \n",
    "        results.append({\n",
    "            \"query_id\": qid,\n",
    "            \"query\": item['query'][:40],\n",
    "            \"hit\": hit,\n",
    "            \"precision\": prec,\n",
    "            \"recall\": rec,\n",
    "            \"first_rank\": first_relevant_rank,\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "analysis = per_query_analysis(golden_dataset, simulated_retrievals, k=3)\n",
    "from tests import checks\n",
    "checks.check_lab_5_4(analysis)\n",
    "\n",
    "print(f\"{'Query':<45} {'Hit':>5} {'P@3':>6} {'R@3':>6} {'1st Rank':>9}\")\n",
    "print(\"-\" * 75)\n",
    "for r in analysis:\n",
    "    hit_str = \"Y\" if r['hit'] else \"N\"\n",
    "    print(f\"{r['query']:<45} {hit_str:>5} {r['precision']:>6.2f} {r['recall']:>6.2f} {r['first_rank']:>9}\")\n",
    "\n",
    "# Identify weakest query\n",
    "worst = min(analysis, key=lambda x: x['recall'] if x['recall'] is not None else 0)\n",
    "print(f\"\\nWeakest query: {worst['query_id']} - '{worst['query']}' (Recall: {worst['recall']})\")\n",
    "print(\"Per-query analysis working!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i1j2k3l4m5",
   "metadata": {},
   "source": [
    "## Part 5: Generation Evaluation Concepts\n",
    "\n",
    "Retrieval metrics tell you whether you found the right documents. But did the LLM **use** those documents correctly to generate an answer?\n",
    "\n",
    "**LLM-as-Judge** is a paradigm where a powerful LLM (e.g., GPT-4) evaluates the quality of another model's output. Key dimensions:\n",
    "\n",
    "- **Faithfulness**: Is the answer grounded in the retrieved context, or does it hallucinate?\n",
    "- **Answer Relevance**: Does the answer actually address the user's question?\n",
    "- **Contextual Precision**: Are the most relevant chunks ranked first in the context window?\n",
    "\n",
    "Below we demonstrate simplified heuristic versions of these metrics. In production, you would use frameworks like **DeepEval** that leverage GPT-4 as a judge for more nuanced evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n6o7p8q9r0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the concept of LLM-as-Judge evaluation\n",
    "# In production: use DeepEval framework\n",
    "\n",
    "def evaluate_faithfulness(answer, context):\n",
    "    \"\"\"\n",
    "    Check if the answer is faithful to the context (no hallucination).\n",
    "    In production: DeepEval's FaithfulnessMetric with GPT-4 as judge.\n",
    "    \"\"\"\n",
    "    # Simple heuristic: check if key phrases from answer appear in context\n",
    "    answer_words = set(answer.lower().split())\n",
    "    context_words = set(context.lower().split())\n",
    "    overlap = len(answer_words & context_words)\n",
    "    score = overlap / max(len(answer_words), 1)\n",
    "    return min(score, 1.0)\n",
    "\n",
    "def evaluate_relevance(answer, query):\n",
    "    \"\"\"\n",
    "    Check if the answer is relevant to the query.\n",
    "    In production: DeepEval's AnswerRelevanceMetric.\n",
    "    \"\"\"\n",
    "    query_words = set(query.lower().split())\n",
    "    answer_words = set(answer.lower().split())\n",
    "    overlap = len(query_words & answer_words)\n",
    "    return min(overlap / max(len(query_words), 1), 1.0)\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"query\": \"What is RAG?\",\n",
    "        \"context\": \"RAG combines retrieval with generation to ground LLM responses in external knowledge, reducing hallucinations.\",\n",
    "        \"answer\": \"RAG (Retrieval-Augmented Generation) combines retrieval with generation to ground responses in external knowledge [1].\",\n",
    "        \"expected\": \"high faithfulness\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is RAG?\",\n",
    "        \"context\": \"RAG combines retrieval with generation.\",\n",
    "        \"answer\": \"RAG was invented by Facebook AI Research in 2020 and uses FAISS for vector search.\",\n",
    "        \"expected\": \"low faithfulness (hallucination)\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Generation Evaluation (Simulated):\")\n",
    "print(\"-\" * 70)\n",
    "for tc in test_cases:\n",
    "    faith = evaluate_faithfulness(tc['answer'], tc['context'])\n",
    "    relev = evaluate_relevance(tc['answer'], tc['query'])\n",
    "    print(f\"Query: {tc['query']}\")\n",
    "    print(f\"Answer: {tc['answer'][:80]}...\")\n",
    "    print(f\"Faithfulness: {faith:.2f} | Relevance: {relev:.2f} | Expected: {tc['expected']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s1t2u3v4w5",
   "metadata": {},
   "source": [
    "### Production Evaluation with DeepEval\n",
    "\n",
    "In production, you would use the **DeepEval** framework for automated evaluation:\n",
    "\n",
    "```python\n",
    "# pip install deepeval\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import FaithfulnessMetric, AnswerRelevanceMetric\n",
    "from deepeval import assert_test\n",
    "\n",
    "faithfulness = FaithfulnessMetric(threshold=0.7, model=\"gpt-4o\")\n",
    "relevance = AnswerRelevanceMetric(threshold=0.7, model=\"gpt-4o\")\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What is RAG?\",\n",
    "    actual_output=\"RAG combines retrieval with generation [1].\",\n",
    "    retrieval_context=[\"RAG combines retrieval with generation...\"]\n",
    ")\n",
    "\n",
    "assert_test(test_case, [faithfulness, relevance])\n",
    "```\n",
    "\n",
    "**Key metrics:**\n",
    "- **Faithfulness**: Is the answer derived purely from the context? (catches hallucinations)\n",
    "- **Answer Relevance**: Does the answer address the question?\n",
    "- **Contextual Precision**: Are relevant chunks ranked highly in the context?\n",
    "\n",
    "> **Note:** DeepEval uses GPT-4 as a judge, which costs ~$0.01-0.05 per evaluation. Budget accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x1y2z3a4b5",
   "metadata": {},
   "source": [
    "### Exercise 6.1: Build a Test Suite\n",
    "\n",
    "Complete the `run_eval_suite` function to evaluate a batch of RAG responses using our simplified faithfulness and relevance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d7e8f9g0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval_suite(rag_responses):\n",
    "    \"\"\"\n",
    "    Run evaluation on a batch of RAG responses.\n",
    "    \n",
    "    Args:\n",
    "        rag_responses: List of dicts with keys: query, answer, context, expected_answer (optional)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for resp in rag_responses:\n",
    "        # TODO: Calculate faithfulness score\n",
    "        faith = None\n",
    "        \n",
    "        # TODO: Calculate relevance score\n",
    "        relev = None\n",
    "        \n",
    "        # TODO: Determine pass/fail using threshold 0.5\n",
    "        passed = None  # True if both scores >= 0.5\n",
    "        \n",
    "        results.append({\n",
    "            \"query\": resp['query'][:40],\n",
    "            \"faithfulness\": faith,\n",
    "            \"relevance\": relev,\n",
    "            \"passed\": passed,\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test data\n",
    "test_responses = [\n",
    "    {\n",
    "        \"query\": \"What is chunking?\",\n",
    "        \"answer\": \"Chunking is splitting documents into smaller pieces for embedding and retrieval.\",\n",
    "        \"context\": \"Chunking is the process of breaking documents into smaller, manageable pieces for embedding and storage in vector databases.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How does HNSW work?\",\n",
    "        \"answer\": \"HNSW builds a multi-layer graph for approximate nearest neighbor search.\",\n",
    "        \"context\": \"HNSW (Hierarchical Navigable Small World) is an algorithm that builds a multi-layered graph structure for fast approximate nearest neighbor search.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "suite_results = run_eval_suite(test_responses)\n",
    "from tests import checks\n",
    "checks.check_lab_5_6(suite_results)\n",
    "\n",
    "print(f\"{'Query':<42} {'Faith':>7} {'Relev':>7} {'Pass':>6}\")\n",
    "print(\"-\" * 65)\n",
    "for r in suite_results:\n",
    "    status = \"PASS\" if r['passed'] else \"FAIL\"\n",
    "    print(f\"{r['query']:<42} {r['faithfulness']:>7.2f} {r['relevance']:>7.2f} {status:>6}\")\n",
    "\n",
    "pass_rate = sum(1 for r in suite_results if r['passed']) / len(suite_results)\n",
    "print(f\"\\nPass rate: {pass_rate:.0%}\")\n",
    "print(\"Eval suite working!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h1i2j3k4l5",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "1. **Metric Choice**: A legal discovery system needs to find ALL relevant precedents. Which metric matters most: Precision@K or Recall@K? Why?\n",
    "2. **Golden Dataset Size**: How many query-document pairs do you need for a reliable evaluation? What are the trade-offs of too few vs too many?\n",
    "3. **LLM-as-Judge**: What are the risks of using GPT-4 to evaluate GPT-4's outputs? How would you mitigate this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m6n7o8p9q0",
   "metadata": {},
   "source": [
    "*Your answers here:*\n",
    "1. ...\n",
    "2. ...\n",
    "3. ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}